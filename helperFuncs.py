import datetime
import dateutil
from dateutil import parser
import requests
from bs4 import BeautifulSoup
import urllib.parse
import tldextract
import re
import sys
import json

def webParser(url):
    
    def scrapeAllTables() :
    	for tableIndex, table in tableEnum:
	        "<tr> tag defines a row in an HTML table."
	        rows = table.find_all('tr')
	        for row in rows:
	            "<td> tag defines a standard data cell in an HTML table"
	            cells = row.find_all('td')
	            cells = [x.text.strip() for x in cells]

	            if tableIndex == 0:
	                bulletin.append(cells)

	            elif tableIndex == 1:
	                try:
	                    vendor= cells[0].lower().split(" ")[0]
	                    product = cells[0].lower().replace(vendor,"").strip()
	                    productName = product
	                    splitCells = cells[1].lower().split()
	                    cpe_dict={
	                        "vendor": vendor,
	                        "product": product,
	                        "category": "a"
	                    }
	                    if splitCells[1] and splitCells[1] != "and":
	                        startVersion = splitCells[-1]
	                        cpe_dict["versionStartIncluding"] = startVersion
	                    endVersion = splitCells[0]
	                    cpe_dict["versionEndIncluding"] = endVersion
	                    cpe_list.append(cpe_dict)
	                except:
	                    continue
	            elif tableIndex == 3:
	                vulnerabilities.append(cells)


    cpe_list=[]
    bulletin=[]
    vulnerabilities=[]

    page = requests.get(url)
    soup = BeautifulSoup(page.content, "html.parser")
    tables = soup.find_all("div", {"class": "table parbase section"})

    tableEnum = enumerate(tables)

    productName = ""
    startVersion = None
    endVersion = None

    scrapeAllTables()

    dateDetails = bulletin[1][1]
    publishedDate = parser.parse(dateDetails).isoformat()

    CVEs = []

    cveIndex = [(vulnerabilities[0].index(x)) for x in vulnerabilities[0] if "CVE" in x]
    vulCategory = [(vulnerabilities[0].index(x)) for x in vulnerabilities[0] if "Vulnerability Category" in x]

    for elem in vulnerabilities[1:]:
        now = datetime.datetime.now()
        timestamp = now.isoformat()

        description = elem[vulCategory[0]]
        id = elem[cveIndex[0]]
        name = productName         
        cve = {
            "timestamp": str(timestamp),
            "published_date": str(publishedDate),
            "id" : str(id),
            "url" : str(url),
            "name" : name,
            "description": str(description).lower(),
            "cpes": {
                "cpe_list" : cpe_list
            }
        }
        CVEs.append(cve)
    

    ext = tldextract.extract(url)
    source = ext.domain            
    typ = "vendor"
    jsonOutput = {
        "source" : source,
        "type" : typ,
        "cves" : CVEs
    }

    outputFile = open('output.json', 'w')
    outputFile.write(json.dumps(jsonOutput, indent=4))
    outputFile.close()


    #return jsonOutput